{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cudf in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (22.8.0)\r\n",
      "Requirement already satisfied: jupyter in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (1.0.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (3.5.3)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 4)) (1.22.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 5)) (1.4.4)\r\n",
      "Requirement already satisfied: pipfile in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 6)) (0.0.2)\r\n",
      "Requirement already satisfied: pyarrow in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 7)) (8.0.1)\r\n",
      "Requirement already satisfied: pyspark in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 8)) (3.3.0)\r\n",
      "Requirement already satisfied: seaborn in /opt/conda/envs/rapids/lib/python3.9/site-packages (from -r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 9)) (0.11.2)\r\n",
      "Requirement already satisfied: cachetools in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (5.2.0)\r\n",
      "Requirement already satisfied: cuda-python<11.7.1,>=11.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (11.7.0)\r\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (2022.8.2)\r\n",
      "Requirement already satisfied: numba>=0.53.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (0.55.2)\r\n",
      "Requirement already satisfied: nvtx>=0.2.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (0.2.3)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (21.3)\r\n",
      "Requirement already satisfied: protobuf<3.21.0a0,>=3.20.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (3.20.1)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (4.3.0)\r\n",
      "Requirement already satisfied: cupy-cuda115<11.0.0a0,>=9.5.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (10.6.0)\r\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (6.4.4)\r\n",
      "Requirement already satisfied: qtconsole in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (5.3.2)\r\n",
      "Requirement already satisfied: ipykernel in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (6.15.2)\r\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (8.0.1)\r\n",
      "Requirement already satisfied: nbconvert in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (7.0.0)\r\n",
      "Requirement already satisfied: notebook in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (6.4.12)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from matplotlib->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (9.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from matplotlib->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from matplotlib->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (0.11.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from matplotlib->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (2.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from matplotlib->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (1.4.4)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from matplotlib->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (4.37.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pandas->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 5)) (2022.2.1)\r\n",
      "Requirement already satisfied: toml in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pipfile->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 6)) (0.10.2)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from pyspark->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 8)) (0.10.9.5)\r\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from seaborn->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 9)) (1.6.0)\r\n",
      "Requirement already satisfied: cython in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cuda-python<11.7.1,>=11.5->cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (0.29.32)\r\n",
      "Requirement already satisfied: fastrlock>=0.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cupy-cuda115<11.0.0a0,>=9.5.0->cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (0.8)\r\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.53.1->cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (0.38.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/rapids/lib/python3.9/site-packages (from numba>=0.53.1->cudf->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 1)) (60.10.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 3)) (1.16.0)\r\n",
      "Requirement already satisfied: debugpy>=1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (1.6.3)\r\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (5.3.0)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (5.9.1)\r\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (23.2.1)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.1.6)\r\n",
      "Requirement already satisfied: tornado>=6.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (6.1)\r\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (1.5.5)\r\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (7.3.4)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (7.31.1)\r\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipywidgets->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (3.0.2)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipywidgets->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (4.0.2)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter-console->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (3.0.30)\r\n",
      "Requirement already satisfied: pygments in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter-console->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (2.13.0)\r\n",
      "Requirement already satisfied: jinja2>=3.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (3.1.2)\r\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (4.11.4)\r\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (4.11.1)\r\n",
      "Requirement already satisfied: mistune<3,>=2.0.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (2.0.4)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (4.11.1)\r\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.2.2)\r\n",
      "Requirement already satisfied: nbformat>=5.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (5.4.0)\r\n",
      "Requirement already satisfied: bleach in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (5.0.1)\r\n",
      "Requirement already satisfied: markupsafe>=2.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (2.1.1)\r\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (1.1.1)\r\n",
      "Requirement already satisfied: defusedxml in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.7.1)\r\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (1.5.0)\r\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.6.7)\r\n",
      "Requirement already satisfied: lxml in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (4.8.0)\r\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/envs/rapids/lib/python3.9/site-packages (from notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.14.1)\r\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (1.8.0)\r\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.15.0)\r\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/envs/rapids/lib/python3.9/site-packages (from notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (21.3.0)\r\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/envs/rapids/lib/python3.9/site-packages (from notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.2.0)\r\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from qtconsole->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (2.2.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from importlib-metadata>=3.6->nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (3.8.1)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.7.5)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.2.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (4.8.0)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (5.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from ipython>=7.23.1->ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.18.1)\r\n",
      "Requirement already satisfied: entrypoints in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.4)\r\n",
      "Requirement already satisfied: fastjsonschema in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbformat>=5.1->nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (2.16.1)\r\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from nbformat>=5.1->nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (4.15.0)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/rapids/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.2.5)\r\n",
      "Requirement already satisfied: ptyprocess in /opt/conda/envs/rapids/lib/python3.9/site-packages (from terminado>=0.8.3->notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.7.0)\r\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/envs/rapids/lib/python3.9/site-packages (from argon2-cffi->notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (21.2.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (2.3.2.post1)\r\n",
      "Requirement already satisfied: webencodings in /opt/conda/envs/rapids/lib/python3.9/site-packages (from bleach->nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.5.1)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.8.3)\r\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (22.1.0)\r\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (0.18.1)\r\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/envs/rapids/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (1.15.1)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/rapids/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->-r /rapids/notebooks/ECD-TCC/jupyter/requirements.txt (line 2)) (2.21)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install \"-r\" \"/rapids/notebooks/ECD-TCC/jupyter/requirements.txt\"\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark SparkSession\n",
    "sparkConf = (\n",
    "    SparkConf()\n",
    "    .setAppName(\"ECD_TCC\")\n",
    "    .setMaster(\"local\")\n",
    "    .setAll([(k, v) for k, v in {\n",
    "        # \"spark.driver.memory\": \"8g\",\n",
    "        # \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.resource.gpu.amount\": \"1\",\n",
    "        \"spark.executor.resource.gpu.discoveryScript\": \"/opt/sparkRapidsPlugin/getGpusResources.sh\",\n",
    "        \"spark.hadoop.parquet.summary.metadata.level\": \"none\",\n",
    "        \"spark.jars\": \"/opt/sparkRapidsPlugin/rapids-4-spark_2.12-22.08.0.jar\",\n",
    "        \"spark.locality.wait\": \"0s\",\n",
    "        \"spark.plugins\": \"com.nvidia.spark.SQLPlugin\",\n",
    "        \"spark.rapids.memory.pinnedPool.size\": \"2G\",\n",
    "        \"spark.rapids.sql.concurrentGpuTasks\": \"1\",\n",
    "        \"spark.sql.files.maxPartitionBytes\": \"512m\",\n",
    "        \"spark.sql.parquet.mergeSchema\": \"false\",\n",
    "        \"spark.sql.cache.serializer\": \"com.nvidia.spark.ParquetCachedBatchSerializer\",\n",
    "        \"spark.rapids.sql.exec.CollectLimitExec\": \"true\"\n",
    "    }.items()])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                 (0 + 1) / 1][Stage 29:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:06:03 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#5664) could run on GPU\n",
      "    !Expression <Max> max(label#5664) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#5664 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#5664)#5954 could run on GPU\n",
      "  @Expression <Alias> max(label#5664)#5954 AS max(label)#5955 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#5664)#5954 could run on GPU\n",
      "\n",
      "22/10/05 18:06:03 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#5664) could run on GPU\n",
      "    !Expression <Max> max(label#5664) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#5664 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#5664)#5954 could run on GPU\n",
      "  @Expression <Alias> max(label#5664)#5954 AS max(label)#5955 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#5664)#5954 could run on GPU\n",
      "\n",
      "22/10/05 18:06:03 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(label#6489, 1.0#6490, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,true), StructField(1.0,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> label#6489 could run on GPU\n",
      "    @Expression <AttributeReference> 1.0#6490 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "  !Expression <AttributeReference> obj#6494 cannot run on GPU because expression AttributeReference obj#6494 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "    @Expression <Alias> cast(label#647 as double) AS label#6489 could run on GPU\n",
      "      @Expression <Cast> cast(label#647 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "    @Expression <Alias> 1.0 AS 1.0#6490 could run on GPU\n",
      "      @Expression <Literal> 1.0 could run on GPU\n",
      "    !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filtrando apenas casos de covid\n",
    "df_raw = spark.read.parquet(\"/rapids/notebooks/ECD-TCC/jupyter/datasets/raw\").where(\"CLASSI_FIN == 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando apenas as colunas com fatores de risco\n",
    "select_cols = [\"PUERPERA\", \"CARDIOPATI\", \"HEMATOLOGI\", \"SIND_DOWN\", \"HEPATICA\", \"ASMA\", \"DIABETES\", \"NEUROLOGIC\",\n",
    "               \"PNEUMOPATI\", \"IMUNODEPRE\", \"RENAL\", \"OBESIDADE\", \"VACINA_COV\", \"VACINA\", \"EVOLUCAO\"]\n",
    "feature_list = select_cols[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:06:03 WARN CacheManager: Asked to cache already cached data.\n",
      "22/10/05 18:06:03 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# Preenchendo valores nulos como ignorados e normalizando os preenchimentos de fatores de risco\n",
    "df = df_raw.select(select_cols).where(\"VACINA_COV <> '12/02/2021'\").cache()\n",
    "for column in feature_list:\n",
    "    df = df.withColumn(\n",
    "        column,\n",
    "        when(col(column) == \"1\", 1)\n",
    "        .when(col(column) == \"2\", -1)\n",
    "        .otherwise(0)\n",
    "    )\n",
    "df = df.withColumn(\"label\", col(\"EVOLUCAO\").cast(IntegerType()) - 1).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset com os dados relevantes\n",
      "22/10/05 18:06:03 WARN GpuOverrides: \n",
      "  ! <AttachDistributedSequenceExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.python.AttachDistributedSequenceExec\n",
      "    @Expression <AttributeReference> __index_level_0__#7204L could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                 (0 + 0) / 1][Stage 35:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:06:07 ERROR Executor: Exception in task 0.0 in stage 35.0 (TID 27)\n",
      "scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/10/05 18:06:07 WARN TaskSetManager: Lost task 0.0 in stage 35.0 (TID 27) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/10/05 18:06:07 ERROR TaskSetManager: Task 0 in stage 35.0 failed 1 times; aborting job\n",
      "22/10/05 18:06:07 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 1 times, most recent failure: Lost task 0.0 in stage 35.0 (TID 27) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": "   PUERPERA  CARDIOPATI  HEMATOLOGI  SIND_DOWN  HEPATICA  ASMA  DIABETES  NEUROLOGIC  PNEUMOPATI  IMUNODEPRE  RENAL  OBESIDADE  VACINA_COV  VACINA EVOLUCAO  label\n0        -1          -1          -1         -1        -1    -1         1          -1          -1          -1     -1         -1           1       0        1    0.0\n1        -1          -1          -1         -1        -1    -1         1          -1          -1          -1     -1         -1           1       0        1    0.0\n2         0           0           0          0         0     0         0           0           0           0      0          0          -1      -1        1    0.0\n3         0           0           0          0         0     0         1           0           0           0      0          0          -1       1     None    NaN\n4         0           1           0          0         0     0         0           0           0           0      0          0           1       1        1    0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PUERPERA</th>\n      <th>CARDIOPATI</th>\n      <th>HEMATOLOGI</th>\n      <th>SIND_DOWN</th>\n      <th>HEPATICA</th>\n      <th>ASMA</th>\n      <th>DIABETES</th>\n      <th>NEUROLOGIC</th>\n      <th>PNEUMOPATI</th>\n      <th>IMUNODEPRE</th>\n      <th>RENAL</th>\n      <th>OBESIDADE</th>\n      <th>VACINA_COV</th>\n      <th>VACINA</th>\n      <th>EVOLUCAO</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>1</td>\n      <td>None</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset com os dados relevantes\")\n",
    "df.pandas_api().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:06:07 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(PUERPERA#423, CARDIOPATI#439, HEMATOLOGI#455, SIND_DOWN#471, HEPATICA#487, ASMA#503, DIABETES#519, NEUROLOGIC#535, PNEUMOPATI#551, IMUNODEPRE#567, RENAL#583, OBESIDADE#599, VACINA_COV#615, VACINA#631, EVOLUCAO#109.toString, label#8052, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, CrossValidator_f495e622d061_rand#2384, StructField(PUERPERA,IntegerType,false), StructField(CARDIOPATI,IntegerType,false), StructField(HEMATOLOGI,IntegerType,false), StructField(SIND_DOWN,IntegerType,false), StructField(HEPATICA,IntegerType,false), StructField(ASMA,IntegerType,false), ... 12 more fields) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "    ! <Invoke> EVOLUCAO#109.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "    @Expression <AttributeReference> label#8052 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "  !Expression <AttributeReference> obj#8071 cannot run on GPU because expression AttributeReference obj#8071 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "    @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "    @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "    @Expression <Alias> cast(label#647 as double) AS label#8052 could run on GPU\n",
      "      @Expression <Cast> cast(label#647 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "    !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "      @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "      @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "      @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "      @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "      @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "      @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "      @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "      @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "      @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "      @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "      @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "      @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "      @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:06:07 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#8052) could run on GPU\n",
      "    !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#8052 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  @Expression <Alias> max(label#8052)#8342 AS max(label)#8343 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#8052) could run on GPU\n",
      "        !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#8052 could run on GPU\n",
      "      @Expression <AttributeReference> max#8615 could run on GPU\n",
      "      @Expression <AttributeReference> max#8616 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:06:07 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#8052) could run on GPU\n",
      "    !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#8052 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  @Expression <Alias> max(label#8052)#8342 AS max(label)#8343 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#8052) could run on GPU\n",
      "        !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#8052 could run on GPU\n",
      "      @Expression <AttributeReference> max#8615 could run on GPU\n",
      "      @Expression <AttributeReference> max#8616 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:06:07 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#8052) could run on GPU\n",
      "    !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#8052 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  @Expression <Alias> max(label#8052)#8342 AS max(label)#8343 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#8052) could run on GPU\n",
      "        !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#8052 could run on GPU\n",
      "      @Expression <AttributeReference> max#8615 could run on GPU\n",
      "      @Expression <AttributeReference> max#8616 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:06:07 WARN GpuOverrides: \n",
      "!Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "    @Expression <AggregateExpression> partial_max(label#8052) could run on GPU\n",
      "      !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "        @Expression <AttributeReference> label#8052 could run on GPU\n",
      "    @Expression <AttributeReference> max#8615 could run on GPU\n",
      "    @Expression <AttributeReference> max#8616 could run on GPU\n",
      "      !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                 (0 + 1) / 1][Stage 39:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:07:21 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#8052) could run on GPU\n",
      "    !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#8052 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  @Expression <Alias> max(label#8052)#8342 AS max(label)#8343 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "\n",
      "22/10/05 18:07:21 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#8052) could run on GPU\n",
      "    !Expression <Max> max(label#8052) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#8052 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "  @Expression <Alias> max(label#8052)#8342 AS max(label)#8343 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#8052)#8342 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Valores de camadas inciais e finais para o MLP\n",
    "features_n = len(select_cols) - 1\n",
    "classes_n = df.select(\"EVOLUCAO\").distinct().count() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:07:22 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(label#8718, 1.0#8719, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,true), StructField(1.0,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> label#8718 could run on GPU\n",
      "    @Expression <AttributeReference> 1.0#8719 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "  !Expression <AttributeReference> obj#8723 cannot run on GPU because expression AttributeReference obj#8723 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "    @Expression <Alias> cast(label#647 as double) AS label#8718 could run on GPU\n",
      "      @Expression <Cast> cast(label#647 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "    @Expression <Alias> 1.0 AS 1.0#8719 could run on GPU\n",
      "      @Expression <Literal> 1.0 could run on GPU\n",
      "    !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Juntar as variveis em um vetor de features\n",
    "assembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\")\n",
    "assembledDF = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelos a testar: regresso logstica, Floresta aleatria e multilayerperceptron\n",
    "lr = LogisticRegression().setFamily(\"binomial\")\n",
    "rf = RandomForestClassifier()\n",
    "mlp = MultilayerPerceptronClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Pipelines de transformaes para a validao cruzada\n",
    "pipeline_lr = Pipeline(stages=[rf])\n",
    "pipeline_rf = Pipeline(stages=[rf])\n",
    "pipeline_mlp = Pipeline(stages=[mlp])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Parmetros para a validao cruzada\n",
    "paramGrid_lr = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5, 1.0, 2.0])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "    .addGrid(lr.maxIter, [1, 5, 10, 20, 50])\n",
    "    .build()\n",
    ")\n",
    "paramGrid_rf = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [10, 30, 50])\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "    .build()\n",
    ")\n",
    "paramGrid_mlp = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(mlp.layers, [\n",
    "        [features_n, 3, 3, classes_n],\n",
    "        [features_n, 4, 4, classes_n],\n",
    "        [features_n, 3, 3, 3, classes_n],\n",
    "        [features_n, 4, 4, 4, classes_n],\n",
    "    ]).build()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Validador cruzado\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=paramGrid_lr,\n",
    "                             evaluator=MulticlassClassificationEvaluator())\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf, estimatorParamMaps=paramGrid_rf,\n",
    "                             evaluator=MulticlassClassificationEvaluator())\n",
    "crossval_mlp = CrossValidator(estimator=pipeline_mlp, estimatorParamMaps=paramGrid_mlp,\n",
    "                              evaluator=MulticlassClassificationEvaluator())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar os dados em treino e teste\n",
    "taxa_de_treino = 0.00001\n",
    "(trainingData, testData) = df.randomSplit([taxa_de_treino, 1 - taxa_de_treino])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                 (0 + 1) / 1][Stage 48:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:07:27 ERROR Executor: Exception in task 0.0 in stage 47.0 (TID 36)\n",
      "scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/10/05 18:07:27 WARN TaskSetManager: Lost task 0.0 in stage 47.0 (TID 36) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/10/05 18:07:27 ERROR TaskSetManager: Task 0 in stage 47.0 failed 1 times; aborting job\n",
      "22/10/05 18:07:27 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 47.0 failed 1 times, most recent failure: Lost task 0.0 in stage 47.0 (TID 36) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "22/10/05 18:07:27 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(PUERPERA#423, CARDIOPATI#439, HEMATOLOGI#455, SIND_DOWN#471, HEPATICA#487, ASMA#503, DIABETES#519, NEUROLOGIC#535, PNEUMOPATI#551, IMUNODEPRE#567, RENAL#583, OBESIDADE#599, VACINA_COV#615, VACINA#631, EVOLUCAO#109.toString, label#9540, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, CrossValidator_f495e622d061_rand#2384, StructField(PUERPERA,IntegerType,false), StructField(CARDIOPATI,IntegerType,false), StructField(HEMATOLOGI,IntegerType,false), StructField(SIND_DOWN,IntegerType,false), StructField(HEPATICA,IntegerType,false), StructField(ASMA,IntegerType,false), ... 12 more fields) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "    ! <Invoke> EVOLUCAO#109.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "    @Expression <AttributeReference> label#9540 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "  !Expression <AttributeReference> obj#9559 cannot run on GPU because expression AttributeReference obj#9559 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "    @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "    @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "    @Expression <Alias> cast(label#647 as double) AS label#9540 could run on GPU\n",
      "      @Expression <Cast> cast(label#647 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "    !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "      @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "      @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "      @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "      @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "      @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "      @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "      @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "      @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "      @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "      @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "      @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "      @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "      @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:07:27 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#9540) could run on GPU\n",
      "    !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#9540 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  @Expression <Alias> max(label#9540)#9830 AS max(label)#9831 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#9540) could run on GPU\n",
      "        !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#9540 could run on GPU\n",
      "      @Expression <AttributeReference> max#10103 could run on GPU\n",
      "      @Expression <AttributeReference> max#10104 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:07:27 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#9540) could run on GPU\n",
      "    !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#9540 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  @Expression <Alias> max(label#9540)#9830 AS max(label)#9831 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#9540) could run on GPU\n",
      "        !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#9540 could run on GPU\n",
      "      @Expression <AttributeReference> max#10103 could run on GPU\n",
      "      @Expression <AttributeReference> max#10104 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:07:27 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#9540) could run on GPU\n",
      "    !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#9540 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  @Expression <Alias> max(label#9540)#9830 AS max(label)#9831 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#9540) could run on GPU\n",
      "        !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#9540 could run on GPU\n",
      "      @Expression <AttributeReference> max#10103 could run on GPU\n",
      "      @Expression <AttributeReference> max#10104 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:07:27 WARN GpuOverrides: \n",
      "!Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "    @Expression <AggregateExpression> partial_max(label#9540) could run on GPU\n",
      "      !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "        @Expression <AttributeReference> label#9540 could run on GPU\n",
      "    @Expression <AttributeReference> max#10103 could run on GPU\n",
      "    @Expression <AttributeReference> max#10104 could run on GPU\n",
      "      !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                 (0 + 1) / 1][Stage 50:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:08:44 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#9540) could run on GPU\n",
      "    !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#9540 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  @Expression <Alias> max(label#9540)#9830 AS max(label)#9831 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "\n",
      "22/10/05 18:08:44 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#9540) could run on GPU\n",
      "    !Expression <Max> max(label#9540) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#9540 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "  @Expression <Alias> max(label#9540)#9830 AS max(label)#9831 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#9540)#9830 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:08:45 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(label#10199, 1.0#10200, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,true), StructField(1.0,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> label#10199 could run on GPU\n",
      "    @Expression <AttributeReference> 1.0#10200 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "  !Expression <AttributeReference> obj#10204 cannot run on GPU because expression AttributeReference obj#10204 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "    @Expression <Alias> cast(label#647 as double) AS label#10199 could run on GPU\n",
      "      @Expression <Cast> cast(label#647 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "    @Expression <Alias> 1.0 AS 1.0#10200 could run on GPU\n",
      "      @Expression <Literal> 1.0 could run on GPU\n",
      "    !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Salvando as massa utilizadas em treino e teste para fazer anlises depois de gerar os modelos\n",
    "trainingData.write.mode(\"overwrite\").parquet(\"/rapids/notebooks/ECD-TCC/jupyter/datasets/training\")\n",
    "testData.write.mode(\"overwrite\").parquet(\"/rapids/notebooks/ECD-TCC/jupyter/datasets/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:08:47 WARN GpuOverrides: \n",
      "!Exec <FilterExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]\n",
      "  @Expression <And> ((CrossValidator_84eacfa9d0dd_rand#10475 >= 0.0) AND (CrossValidator_84eacfa9d0dd_rand#10475 < 0.3333333333333333)) could run on GPU\n",
      "    @Expression <GreaterThanOrEqual> (CrossValidator_84eacfa9d0dd_rand#10475 >= 0.0) could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "      @Expression <Literal> 0.0 could run on GPU\n",
      "    @Expression <LessThan> (CrossValidator_84eacfa9d0dd_rand#10475 < 0.3333333333333333) could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "      @Expression <Literal> 0.3333333333333333 could run on GPU\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; not all expressions can be replaced\n",
      "    @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "    @Expression <AttributeReference> EVOLUCAO#6432 could run on GPU\n",
      "    @Expression <AttributeReference> label#7171 could run on GPU\n",
      "    !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <Alias> rand(6524969372291673858) AS CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "      @Expression <Rand> rand(6524969372291673858) could run on GPU\n",
      "        @Expression <Literal> 6524969372291673858 could run on GPU\n",
      "    !Exec <ProjectExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; not all expressions can be replaced\n",
      "      @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "      @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "      @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "      @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "      @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "      @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "      @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "      @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "      @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "      @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "      @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "      @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "      @Expression <AttributeReference> EVOLUCAO#6432 could run on GPU\n",
      "      @Expression <AttributeReference> label#7171 could run on GPU\n",
      "      !Expression <Alias> UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) AS features#9009 cannot run on GPU because expression Alias UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) AS features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7; input expression ScalaUDF UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) (org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 is not supported)\n",
      "        !Expression <ScalaUDF> UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) cannot run on GPU because expression ScalaUDF UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7; neither UDF implemented by class org.apache.spark.ml.feature.VectorAssembler$$Lambda$4368/1938736812 provides a GPU implementation, nor the conf `spark.rapids.sql.rowBasedUDF.enabled` is enabled\n",
      "          @Expression <CreateNamedStruct> struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields) could run on GPU\n",
      "            @Expression <Literal> PUERPERA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(PUERPERA#6947 as double) could run on GPU\n",
      "              @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "            @Expression <Literal> CARDIOPATI_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(CARDIOPATI#6963 as double) could run on GPU\n",
      "              @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "            @Expression <Literal> HEMATOLOGI_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(HEMATOLOGI#6979 as double) could run on GPU\n",
      "              @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "            @Expression <Literal> SIND_DOWN_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(SIND_DOWN#6995 as double) could run on GPU\n",
      "              @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "            @Expression <Literal> HEPATICA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(HEPATICA#7011 as double) could run on GPU\n",
      "              @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "            @Expression <Literal> ASMA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(ASMA#7027 as double) could run on GPU\n",
      "              @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "            @Expression <Literal> DIABETES_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(DIABETES#7043 as double) could run on GPU\n",
      "              @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "            @Expression <Literal> NEUROLOGIC_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(NEUROLOGIC#7059 as double) could run on GPU\n",
      "              @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "            @Expression <Literal> PNEUMOPATI_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(PNEUMOPATI#7075 as double) could run on GPU\n",
      "              @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "            @Expression <Literal> IMUNODEPRE_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(IMUNODEPRE#7091 as double) could run on GPU\n",
      "              @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "            @Expression <Literal> RENAL_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(RENAL#7107 as double) could run on GPU\n",
      "              @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "            @Expression <Literal> OBESIDADE_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(OBESIDADE#7123 as double) could run on GPU\n",
      "              @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "            @Expression <Literal> VACINA_COV_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(VACINA_COV#7139 as double) could run on GPU\n",
      "              @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "            @Expression <Literal> VACINA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(VACINA#7155 as double) could run on GPU\n",
      "              @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "\n",
      "22/10/05 18:08:47 WARN GpuOverrides: \n",
      "!Exec <FilterExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]\n",
      "  @Expression <Or> ((CrossValidator_84eacfa9d0dd_rand#10475 < 0.0) OR (CrossValidator_84eacfa9d0dd_rand#10475 >= 0.3333333333333333)) could run on GPU\n",
      "    @Expression <LessThan> (CrossValidator_84eacfa9d0dd_rand#10475 < 0.0) could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "      @Expression <Literal> 0.0 could run on GPU\n",
      "    @Expression <GreaterThanOrEqual> (CrossValidator_84eacfa9d0dd_rand#10475 >= 0.3333333333333333) could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "      @Expression <Literal> 0.3333333333333333 could run on GPU\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; not all expressions can be replaced\n",
      "    @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "    @Expression <AttributeReference> EVOLUCAO#6432 could run on GPU\n",
      "    @Expression <AttributeReference> label#7171 could run on GPU\n",
      "    !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <Alias> rand(6524969372291673858) AS CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "      @Expression <Rand> rand(6524969372291673858) could run on GPU\n",
      "        @Expression <Literal> 6524969372291673858 could run on GPU\n",
      "    !Exec <ProjectExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; not all expressions can be replaced\n",
      "      @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "      @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "      @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "      @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "      @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "      @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "      @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "      @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "      @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "      @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "      @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "      @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "      @Expression <AttributeReference> EVOLUCAO#6432 could run on GPU\n",
      "      @Expression <AttributeReference> label#7171 could run on GPU\n",
      "      !Expression <Alias> UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) AS features#9009 cannot run on GPU because expression Alias UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) AS features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7; input expression ScalaUDF UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) (org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 is not supported)\n",
      "        !Expression <ScalaUDF> UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) cannot run on GPU because expression ScalaUDF UDF(struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields)) produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7; neither UDF implemented by class org.apache.spark.ml.feature.VectorAssembler$$Lambda$4368/1938736812 provides a GPU implementation, nor the conf `spark.rapids.sql.rowBasedUDF.enabled` is enabled\n",
      "          @Expression <CreateNamedStruct> struct(PUERPERA_double_VectorAssembler_84c04e830732, cast(PUERPERA#6947 as double), CARDIOPATI_double_VectorAssembler_84c04e830732, cast(CARDIOPATI#6963 as double), HEMATOLOGI_double_VectorAssembler_84c04e830732, cast(HEMATOLOGI#6979 as double), SIND_DOWN_double_VectorAssembler_84c04e830732, cast(SIND_DOWN#6995 as double), HEPATICA_double_VectorAssembler_84c04e830732, cast(HEPATICA#7011 as double), ASMA_double_VectorAssembler_84c04e830732, cast(ASMA#7027 as double), DIABETES_double_VectorAssembler_84c04e830732, cast(DIABETES#7043 as double), NEUROLOGIC_double_VectorAssembler_84c04e830732, cast(NEUROLOGIC#7059 as double), PNEUMOPATI_double_VectorAssembler_84c04e830732, cast(PNEUMOPATI#7075 as double), IMUNODEPRE_double_VectorAssembler_84c04e830732, cast(IMUNODEPRE#7091 as double), RENAL_double_VectorAssembler_84c04e830732, cast(RENAL#7107 as double), OBESIDADE_double_VectorAssembler_84c04e830732, cast(OBESIDADE#7123 as double), ... 4 more fields) could run on GPU\n",
      "            @Expression <Literal> PUERPERA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(PUERPERA#6947 as double) could run on GPU\n",
      "              @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "            @Expression <Literal> CARDIOPATI_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(CARDIOPATI#6963 as double) could run on GPU\n",
      "              @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "            @Expression <Literal> HEMATOLOGI_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(HEMATOLOGI#6979 as double) could run on GPU\n",
      "              @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "            @Expression <Literal> SIND_DOWN_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(SIND_DOWN#6995 as double) could run on GPU\n",
      "              @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "            @Expression <Literal> HEPATICA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(HEPATICA#7011 as double) could run on GPU\n",
      "              @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "            @Expression <Literal> ASMA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(ASMA#7027 as double) could run on GPU\n",
      "              @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "            @Expression <Literal> DIABETES_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(DIABETES#7043 as double) could run on GPU\n",
      "              @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "            @Expression <Literal> NEUROLOGIC_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(NEUROLOGIC#7059 as double) could run on GPU\n",
      "              @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "            @Expression <Literal> PNEUMOPATI_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(PNEUMOPATI#7075 as double) could run on GPU\n",
      "              @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "            @Expression <Literal> IMUNODEPRE_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(IMUNODEPRE#7091 as double) could run on GPU\n",
      "              @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "            @Expression <Literal> RENAL_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(RENAL#7107 as double) could run on GPU\n",
      "              @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "            @Expression <Literal> OBESIDADE_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(OBESIDADE#7123 as double) could run on GPU\n",
      "              @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "            @Expression <Literal> VACINA_COV_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(VACINA_COV#7139 as double) could run on GPU\n",
      "              @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "            @Expression <Literal> VACINA_double_VectorAssembler_84c04e830732 could run on GPU\n",
      "            @Expression <Cast> cast(VACINA#7155 as double) could run on GPU\n",
      "              @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:08:47 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(PUERPERA#6947, CARDIOPATI#6963, HEMATOLOGI#6979, SIND_DOWN#6995, HEPATICA#7011, ASMA#7027, DIABETES#7043, NEUROLOGIC#7059, PNEUMOPATI#7075, IMUNODEPRE#7091, RENAL#7107, OBESIDADE#7123, VACINA_COV#7139, VACINA#7155, EVOLUCAO#6432.toString, label#11155, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, CrossValidator_84eacfa9d0dd_rand#10475, StructField(PUERPERA,IntegerType,false), StructField(CARDIOPATI,IntegerType,false), StructField(HEMATOLOGI,IntegerType,false), StructField(SIND_DOWN,IntegerType,false), StructField(HEPATICA,IntegerType,false), StructField(ASMA,IntegerType,false), ... 12 more fields) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "    ! <Invoke> EVOLUCAO#6432.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      @Expression <AttributeReference> EVOLUCAO#6432 could run on GPU\n",
      "    @Expression <AttributeReference> label#11155 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "  !Expression <AttributeReference> obj#11174 cannot run on GPU because expression AttributeReference obj#11174 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; not all expressions can be replaced\n",
      "    @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "    @Expression <AttributeReference> EVOLUCAO#6432 could run on GPU\n",
      "    @Expression <Alias> cast(label#7171 as double) AS label#11155 could run on GPU\n",
      "      @Expression <Cast> cast(label#7171 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#7171 could run on GPU\n",
      "    !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; not all expressions can be replaced\n",
      "      @Expression <AttributeReference> ASMA#7027 could run on GPU\n",
      "      @Expression <AttributeReference> CARDIOPATI#6963 could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_84eacfa9d0dd_rand#10475 could run on GPU\n",
      "      @Expression <AttributeReference> DIABETES#7043 could run on GPU\n",
      "      @Expression <AttributeReference> EVOLUCAO#6432 could run on GPU\n",
      "      @Expression <AttributeReference> HEMATOLOGI#6979 could run on GPU\n",
      "      @Expression <AttributeReference> HEPATICA#7011 could run on GPU\n",
      "      @Expression <AttributeReference> IMUNODEPRE#7091 could run on GPU\n",
      "      @Expression <AttributeReference> NEUROLOGIC#7059 could run on GPU\n",
      "      @Expression <AttributeReference> OBESIDADE#7123 could run on GPU\n",
      "      @Expression <AttributeReference> PNEUMOPATI#7075 could run on GPU\n",
      "      @Expression <AttributeReference> PUERPERA#6947 could run on GPU\n",
      "      @Expression <AttributeReference> RENAL#7107 could run on GPU\n",
      "      @Expression <AttributeReference> SIND_DOWN#6995 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA#7155 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA_COV#7139 could run on GPU\n",
      "      !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#7171 could run on GPU\n",
      "\n",
      "22/10/05 18:08:47 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11155) could run on GPU\n",
      "    !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11155 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  @Expression <Alias> max(label#11155)#11445 AS max(label)#11446 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#11155) could run on GPU\n",
      "        !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#11155 could run on GPU\n",
      "      @Expression <AttributeReference> max#11718 could run on GPU\n",
      "      @Expression <AttributeReference> max#11719 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#7171 could run on GPU\n",
      "\n",
      "22/10/05 18:08:47 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11155) could run on GPU\n",
      "    !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11155 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  @Expression <Alias> max(label#11155)#11445 AS max(label)#11446 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#11155) could run on GPU\n",
      "        !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#11155 could run on GPU\n",
      "      @Expression <AttributeReference> max#11718 could run on GPU\n",
      "      @Expression <AttributeReference> max#11719 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#7171 could run on GPU\n",
      "\n",
      "22/10/05 18:08:47 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11155) could run on GPU\n",
      "    !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11155 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  @Expression <Alias> max(label#11155)#11445 AS max(label)#11446 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#11155) could run on GPU\n",
      "        !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#11155 could run on GPU\n",
      "      @Expression <AttributeReference> max#11718 could run on GPU\n",
      "      @Expression <AttributeReference> max#11719 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#7171 could run on GPU\n",
      "\n",
      "22/10/05 18:08:47 WARN GpuOverrides: \n",
      "!Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "    @Expression <AggregateExpression> partial_max(label#11155) could run on GPU\n",
      "      !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "        @Expression <AttributeReference> label#11155 could run on GPU\n",
      "    @Expression <AttributeReference> max#11718 could run on GPU\n",
      "    @Expression <AttributeReference> max#11719 could run on GPU\n",
      "      !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "        @Expression <AttributeReference> label#7171 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                 (0 + 1) / 1][Stage 56:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:08:48 ERROR Executor: Exception in task 0.0 in stage 55.0 (TID 43)\n",
      "scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/10/05 18:08:48 WARN TaskSetManager: Lost task 0.0 in stage 55.0 (TID 43) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/10/05 18:08:48 ERROR TaskSetManager: Task 0 in stage 55.0 failed 1 times; aborting job\n",
      "22/10/05 18:08:48 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 55.0 failed 1 times, most recent failure: Lost task 0.0 in stage 55.0 (TID 43) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "22/10/05 18:08:49 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(PUERPERA#423, CARDIOPATI#439, HEMATOLOGI#455, SIND_DOWN#471, HEPATICA#487, ASMA#503, DIABETES#519, NEUROLOGIC#535, PNEUMOPATI#551, IMUNODEPRE#567, RENAL#583, OBESIDADE#599, VACINA_COV#615, VACINA#631, EVOLUCAO#109.toString, label#11815, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, CrossValidator_f495e622d061_rand#2384, StructField(PUERPERA,IntegerType,false), StructField(CARDIOPATI,IntegerType,false), StructField(HEMATOLOGI,IntegerType,false), StructField(SIND_DOWN,IntegerType,false), StructField(HEPATICA,IntegerType,false), StructField(ASMA,IntegerType,false), ... 12 more fields) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "    ! <Invoke> EVOLUCAO#109.toString cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "    @Expression <AttributeReference> label#11815 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "  !Expression <AttributeReference> obj#11834 cannot run on GPU because expression AttributeReference obj#11834 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "    @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "    @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "    @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "    @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "    @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "    @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "    @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "    @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "    @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "    @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "    @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "    @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "    @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "    @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "    @Expression <Alias> cast(label#647 as double) AS label#11815 could run on GPU\n",
      "      @Expression <Cast> cast(label#647 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "    !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "      @Expression <AttributeReference> ASMA#503 could run on GPU\n",
      "      @Expression <AttributeReference> CARDIOPATI#439 could run on GPU\n",
      "      @Expression <AttributeReference> CrossValidator_f495e622d061_rand#2384 could run on GPU\n",
      "      @Expression <AttributeReference> DIABETES#519 could run on GPU\n",
      "      @Expression <AttributeReference> EVOLUCAO#109 could run on GPU\n",
      "      @Expression <AttributeReference> HEMATOLOGI#455 could run on GPU\n",
      "      @Expression <AttributeReference> HEPATICA#487 could run on GPU\n",
      "      @Expression <AttributeReference> IMUNODEPRE#567 could run on GPU\n",
      "      @Expression <AttributeReference> NEUROLOGIC#535 could run on GPU\n",
      "      @Expression <AttributeReference> OBESIDADE#599 could run on GPU\n",
      "      @Expression <AttributeReference> PNEUMOPATI#551 could run on GPU\n",
      "      @Expression <AttributeReference> PUERPERA#423 could run on GPU\n",
      "      @Expression <AttributeReference> RENAL#583 could run on GPU\n",
      "      @Expression <AttributeReference> SIND_DOWN#471 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA#631 could run on GPU\n",
      "      @Expression <AttributeReference> VACINA_COV#615 could run on GPU\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:08:49 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11815) could run on GPU\n",
      "    !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11815 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  @Expression <Alias> max(label#11815)#12105 AS max(label)#12106 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#11815) could run on GPU\n",
      "        !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#11815 could run on GPU\n",
      "      @Expression <AttributeReference> max#12378 could run on GPU\n",
      "      @Expression <AttributeReference> max#12379 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:08:49 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11815) could run on GPU\n",
      "    !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11815 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  @Expression <Alias> max(label#11815)#12105 AS max(label)#12106 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#11815) could run on GPU\n",
      "        !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#11815 could run on GPU\n",
      "      @Expression <AttributeReference> max#12378 could run on GPU\n",
      "      @Expression <AttributeReference> max#12379 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:08:49 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11815) could run on GPU\n",
      "    !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11815 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  @Expression <Alias> max(label#11815)#12105 AS max(label)#12106 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  !Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "    @Partitioning <SinglePartition$> could run on GPU\n",
      "    !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "      @Expression <AggregateExpression> partial_max(label#11815) could run on GPU\n",
      "        !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "          @Expression <AttributeReference> label#11815 could run on GPU\n",
      "      @Expression <AttributeReference> max#12378 could run on GPU\n",
      "      @Expression <AttributeReference> max#12379 could run on GPU\n",
      "        !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "          @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n",
      "22/10/05 18:08:49 WARN GpuOverrides: \n",
      "!Exec <ShuffleExchangeExec> cannot run on GPU because Columnar exchange without columnar children is inefficient\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  !Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "    @Expression <AggregateExpression> partial_max(label#11815) could run on GPU\n",
      "      !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "        @Expression <AttributeReference> label#11815 could run on GPU\n",
      "    @Expression <AttributeReference> max#12378 could run on GPU\n",
      "    @Expression <AttributeReference> max#12379 could run on GPU\n",
      "      !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                 (0 + 1) / 1][Stage 57:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:09:51 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11155) could run on GPU\n",
      "    !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11155 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  @Expression <Alias> max(label#11155)#11445 AS max(label)#11446 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "\n",
      "22/10/05 18:09:51 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11155) could run on GPU\n",
      "    !Expression <Max> max(label#11155) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11155 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "  @Expression <Alias> max(label#11155)#11445 AS max(label)#11446 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11155)#11445 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 57:>                 (0 + 1) / 1][Stage 59:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:10:53 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11815) could run on GPU\n",
      "    !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11815 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  @Expression <Alias> max(label#11815)#12105 AS max(label)#12106 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "\n",
      "22/10/05 18:10:53 WARN GpuOverrides: \n",
      "!Exec <HashAggregateExec> cannot run on GPU because not all expressions can be replaced\n",
      "  @Expression <AggregateExpression> max(label#11815) could run on GPU\n",
      "    !Expression <Max> max(label#11815) cannot run on GPU because Max aggregation on floating point columns that can contain NaNs will compute incorrect results. If it is known that there are no NaNs, set  spark.rapids.sql.hasNans to false.\n",
      "      @Expression <AttributeReference> label#11815 could run on GPU\n",
      "  @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "  @Expression <Alias> max(label#11815)#12105 AS max(label)#12106 could run on GPU\n",
      "    @Expression <AttributeReference> max(label#11815)#12105 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:10:53 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(label#12474, 1.0#12477, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,true), StructField(1.0,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> label#12474 could run on GPU\n",
      "    @Expression <AttributeReference> 1.0#12477 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "  !Expression <AttributeReference> obj#12484 cannot run on GPU because expression AttributeReference obj#12484 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; not all expressions can be replaced\n",
      "    @Expression <Alias> cast(label#7171 as double) AS label#12474 could run on GPU\n",
      "      @Expression <Cast> cast(label#7171 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#7171 could run on GPU\n",
      "    @Expression <Alias> 1.0 AS 1.0#12477 could run on GPU\n",
      "      @Expression <Literal> 1.0 could run on GPU\n",
      "    !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#9009]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; not all expressions can be replaced\n",
      "      !Expression <AttributeReference> features#9009 cannot run on GPU because expression AttributeReference features#9009 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#7171 could run on GPU\n",
      "\n",
      "22/10/05 18:10:53 WARN GpuOverrides: \n",
      "! <DeserializeToObjectExec> cannot run on GPU because not all expressions can be replaced; GPU does not currently support the operator class org.apache.spark.sql.execution.DeserializeToObjectExec\n",
      "  ! <CreateExternalRow> createexternalrow(label#12475, 1.0#12476, newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize, StructField(label,DoubleType,true), StructField(1.0,DoubleType,false), StructField(features,org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7,true)) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.CreateExternalRow\n",
      "    @Expression <AttributeReference> label#12475 could run on GPU\n",
      "    @Expression <AttributeReference> 1.0#12476 could run on GPU\n",
      "    ! <Invoke> newInstance(class org.apache.spark.ml.linalg.VectorUDT).deserialize cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.Invoke\n",
      "      ! <NewInstance> newInstance(class org.apache.spark.ml.linalg.VectorUDT) cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.catalyst.expressions.objects.NewInstance\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "  !Expression <AttributeReference> obj#12485 cannot run on GPU because expression AttributeReference obj#12485 produces an unsupported type ObjectType(interface org.apache.spark.sql.Row)\n",
      "  !Exec <ProjectExec> cannot run on GPU because unsupported data types in input: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "    @Expression <Alias> cast(label#647 as double) AS label#12475 could run on GPU\n",
      "      @Expression <Cast> cast(label#647 as double) could run on GPU\n",
      "        @Expression <AttributeReference> label#647 could run on GPU\n",
      "    @Expression <Alias> 1.0 AS 1.0#12476 could run on GPU\n",
      "      @Expression <Literal> 1.0 could run on GPU\n",
      "    !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "    !Exec <InMemoryTableScanExec> cannot run on GPU because unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features]; unsupported data types in output: org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 [features#1854]; not all expressions can be replaced\n",
      "      !Expression <AttributeReference> features#1854 cannot run on GPU because expression AttributeReference features#1854 produces an unsupported type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7\n",
      "      @Expression <AttributeReference> label#647 could run on GPU\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                 (0 + 1) / 1][Stage 65:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/05 18:11:00 ERROR Executor: Exception in task 0.0 in stage 64.0 (TID 50)\n",
      "scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/10/05 18:11:00 WARN TaskSetManager: Lost task 0.0 in stage 64.0 (TID 50) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/10/05 18:11:00 ERROR TaskSetManager: Task 0 in stage 64.0 failed 1 times; aborting job\n",
      "22/10/05 18:11:00 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 50) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n",
      "\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n",
      "\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n",
      "\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1869.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 50) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_2278/60540869.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcrossval_lr_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcrossval_lr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0massembledDF\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             raise TypeError(\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/tuning.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    845\u001B[0m                 \u001B[0m_parallelFitTasks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meva\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    846\u001B[0m             )\n\u001B[0;32m--> 847\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubModel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimap_unordered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    848\u001B[0m                 \u001B[0mmetrics_all\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    849\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/multiprocessing/pool.py\u001B[0m in \u001B[0;36mnext\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    868\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    869\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 870\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    871\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    872\u001B[0m     \u001B[0m__next__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m                    \u001B[0;31m# XXX\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/multiprocessing/pool.py\u001B[0m in \u001B[0;36mworker\u001B[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001B[0m\n\u001B[1;32m    123\u001B[0m         \u001B[0mjob\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    124\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 125\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    126\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mwrap_exception\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mfunc\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0m_helper_reraises_exception\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/tuning.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(f)\u001B[0m\n\u001B[1;32m    845\u001B[0m                 \u001B[0m_parallelFitTasks\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mest\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meva\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepm\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    846\u001B[0m             )\n\u001B[0;32m--> 847\u001B[0;31m             \u001B[0;32mfor\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msubModel\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimap_unordered\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    848\u001B[0m                 \u001B[0mmetrics_all\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    849\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcollectSubModelsParam\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/util.py\u001B[0m in \u001B[0;36mwrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    335\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    336\u001B[0m             \u001B[0mSparkContext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_active_spark_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jsc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msetLocalProperties\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mproperties\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 337\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    338\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    339\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/tuning.py\u001B[0m in \u001B[0;36msingleTask\u001B[0;34m()\u001B[0m\n\u001B[1;32m    111\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msingleTask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mTransformer\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m         \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodelIter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m         \u001B[0;31m# TODO: duplicate evaluator to take extra params from input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m         \u001B[0;31m#  Note: Supporting tuning params in evaluator need update method\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     96\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"No models remaining.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     97\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcounter\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 98\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfitSingleModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mint\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mM\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfitSingleModel\u001B[0;34m(index)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mfitSingleModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 156\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mestimator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparamMaps\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    157\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_FitMultipleIterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfitSingleModel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparamMaps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    201\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 203\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    205\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/pipeline.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    132\u001B[0m                     \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# must be an Estimator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 134\u001B[0;31m                     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    135\u001B[0m                     \u001B[0mtransformers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    136\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mindexOfLastEstimator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             raise TypeError(\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 379\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    380\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    375\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 376\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    377\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 190\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    191\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    192\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/conda/envs/rapids/lib/python3.9/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1869.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 50) (ec49e0c80609 executor driver): scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1200)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1193)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: scala.MatchError: [null,1.0,(14,[1,12],[1.0,-1.0])] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$2(RDD.scala:1198)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$6(SparkContext.scala:2322)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "crossval_lr_model = crossval_lr.fit(assembledDF)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crossval_lr_model.write().save(\"/rapids/notebooks/ECD-TCC/jupyter/model/lr\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o modelo\n",
    "predictions = cvModel.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "predictions.select(\"label\", \"features\", \"rawPrediction\",\n",
    "                   \"probability\", \"prediction\").show()\n",
    "predictions.select(\"prediction\").distinct().show()\n",
    "\n",
    "result = predictions.toPandas()\n",
    "\n",
    "plt.plot(result.label, result.prediction, 'bo')\n",
    "plt.xlabel('Sobrevivencia')\n",
    "plt.ylabel('Prediction')\n",
    "plt.suptitle(\"Model Performance RMSE: %f\" % rmse)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando o melhor modelo\n",
    "bestPipeline = cvModel.bestModel\n",
    "bestModel = bestPipeline.stages[2]\n",
    "\n",
    "importances = bestModel.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "plt.bar(x_values, importances, orientation='vertical')\n",
    "plt.xticks(x_values, feature_list, rotation=40)\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('numTrees - ', bestModel.getNumTrees)\n",
    "print('maxDepth - ', bestModel.getOrDefault('maxDepth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
