{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:35.350185800Z",
     "start_time": "2023-08-26T14:29:35.323626300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.sql.types import IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:35.404401500Z",
     "start_time": "2023-08-26T14:29:35.331131200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create PySpark SparkSession\n",
    "conf: SparkConf = SparkConf().setAppName(\"ECD_TCC\").setMaster(\"local[*]\")\n",
    "spark: SparkSession = SparkSession.builder.config(conf=conf).getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:35.592380500Z",
     "start_time": "2023-08-26T14:29:35.346673900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filtrando apenas casos de covid\n",
    "df_raw = spark.read.option(\"delimiter\", \";\").option(\"header\", True).csv(\"../datasets/raw\").where(\"CLASSI_FIN == 5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:35.608906700Z",
     "start_time": "2023-08-26T14:29:35.595383100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selecionando apenas as colunas com fatores de risco\n",
    "select_cols = [\"PUERPERA\", \"CARDIOPATI\", \"HEMATOLOGI\", \"SIND_DOWN\", \"HEPATICA\", \"ASMA\", \"DIABETES\", \"NEUROLOGIC\",\n",
    "               \"PNEUMOPATI\", \"IMUNODEPRE\", \"RENAL\", \"OBESIDADE\", \"VACINA_COV\", \"VACINA\", \"EVOLUCAO\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:35.814352200Z",
     "start_time": "2023-08-26T14:29:35.609905600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preenchendo valores nulos como ignorados e normalizando os preenchimentos de fatores de risco\n",
    "df = df_raw.select(select_cols).where(\"VACINA_COV <> '12/02/2021'\").cache()\n",
    "for column in select_cols:\n",
    "    df = df.withColumn(\n",
    "        column,\n",
    "        when(col(column) == \"1\", \"Y\")\n",
    "        .when(col(column) == \"2\", \"N\")\n",
    "    )\n",
    "df = df.withColumnRenamed(\"EVOLUCAO\", \"label\").where(col(\"label\").isNotNull()).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:36.264719400Z",
     "start_time": "2023-08-26T14:29:35.815351600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset com os dados relevantes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PUERPERA</th>\n",
       "      <th>CARDIOPATI</th>\n",
       "      <th>HEMATOLOGI</th>\n",
       "      <th>SIND_DOWN</th>\n",
       "      <th>HEPATICA</th>\n",
       "      <th>ASMA</th>\n",
       "      <th>DIABETES</th>\n",
       "      <th>NEUROLOGIC</th>\n",
       "      <th>PNEUMOPATI</th>\n",
       "      <th>IMUNODEPRE</th>\n",
       "      <th>RENAL</th>\n",
       "      <th>OBESIDADE</th>\n",
       "      <th>VACINA_COV</th>\n",
       "      <th>VACINA</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>None</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "      <td>None</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PUERPERA CARDIOPATI HEMATOLOGI SIND_DOWN HEPATICA  ASMA DIABETES NEUROLOGIC  \\\n",
       "0     None       None       None      None     None  None        Y       None   \n",
       "1     None          Y       None      None     None     Y     None       None   \n",
       "2        N          N          N         N        N     N        Y          N   \n",
       "3        N          N          N         N        N     N        Y          N   \n",
       "4     None       None       None      None     None  None     None       None   \n",
       "\n",
       "  PNEUMOPATI IMUNODEPRE RENAL OBESIDADE VACINA_COV VACINA label  \n",
       "0       None       None  None      None          Y      Y     Y  \n",
       "1          Y       None  None      None       None   None     Y  \n",
       "2          Y          N     N         Y          N      N     Y  \n",
       "3          N          N     N         N          N   None     N  \n",
       "4       None       None  None      None          Y   None     Y  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dataset com os dados relevantes\")\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.to_csv(\"../datasets/processed/df.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:37.071971400Z",
     "start_time": "2023-08-26T14:29:36.265719Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'EVOLUCAO' does not exist. Did you mean one of the following? [HEPATICA, VACINA, ASMA, NEUROLOGIC, RENAL, DIABETES, HEMATOLOGI, OBESIDADE, PUERPERA, VACINA_COV, label, CARDIOPATI, IMUNODEPRE, PNEUMOPATI, SIND_DOWN];\n'Project ['EVOLUCAO]\n+- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, VACINA_COV#632, VACINA#648, EVOLUCAO#664 AS label#680]\n   +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, VACINA_COV#632, VACINA#648, CASE WHEN (EVOLUCAO#126 = 1) THEN Y WHEN (EVOLUCAO#126 = 2) THEN N END AS EVOLUCAO#664]\n      +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, VACINA_COV#632, CASE WHEN (VACINA#73 = 1) THEN Y WHEN (VACINA#73 = 2) THEN N END AS VACINA#648, EVOLUCAO#126]\n         +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, CASE WHEN (VACINA_COV#171 = 1) THEN Y WHEN (VACINA_COV#171 = 2) THEN N END AS VACINA_COV#632, VACINA#73, EVOLUCAO#126]\n            +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, CASE WHEN (OBESIDADE#69 = 1) THEN Y WHEN (OBESIDADE#69 = 2) THEN N END AS OBESIDADE#616, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n               +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, CASE WHEN (RENAL#68 = 1) THEN Y WHEN (RENAL#68 = 2) THEN N END AS RENAL#600, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                  +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, CASE WHEN (IMUNODEPRE#67 = 1) THEN Y WHEN (IMUNODEPRE#67 = 2) THEN N END AS IMUNODEPRE#584, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                     +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, CASE WHEN (PNEUMOPATI#66 = 1) THEN Y WHEN (PNEUMOPATI#66 = 2) THEN N END AS PNEUMOPATI#568, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                        +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, CASE WHEN (NEUROLOGIC#65 = 1) THEN Y WHEN (NEUROLOGIC#65 = 2) THEN N END AS NEUROLOGIC#552, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                           +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, CASE WHEN (DIABETES#64 = 1) THEN Y WHEN (DIABETES#64 = 2) THEN N END AS DIABETES#536, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                              +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, CASE WHEN (ASMA#63 = 1) THEN Y WHEN (ASMA#63 = 2) THEN N END AS ASMA#520, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                 +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, CASE WHEN (HEPATICA#62 = 1) THEN Y WHEN (HEPATICA#62 = 2) THEN N END AS HEPATICA#504, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                    +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, CASE WHEN (SIND_DOWN#61 = 1) THEN Y WHEN (SIND_DOWN#61 = 2) THEN N END AS SIND_DOWN#488, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                       +- Project [PUERPERA#440, CARDIOPATI#456, CASE WHEN (HEMATOLOGI#60 = 1) THEN Y WHEN (HEMATOLOGI#60 = 2) THEN N END AS HEMATOLOGI#472, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                          +- Project [PUERPERA#440, CASE WHEN (CARDIOPATI#59 = 1) THEN Y WHEN (CARDIOPATI#59 = 2) THEN N END AS CARDIOPATI#456, HEMATOLOGI#60, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                             +- Project [CASE WHEN (PUERPERA#57 = 1) THEN Y WHEN (PUERPERA#57 = 2) THEN N END AS PUERPERA#440, CARDIOPATI#59, HEMATOLOGI#60, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                                +- Filter NOT (VACINA_COV#171 = 12/02/2021)\n                                                   +- Project [PUERPERA#57, CARDIOPATI#59, HEMATOLOGI#60, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                                      +- Filter (cast(CLASSI_FIN#123 as int) = 5)\n                                                         +- Relation [DT_NOTIFIC#17,SEM_NOT#18,DT_SIN_PRI#19,SEM_PRI#20,SG_UF_NOT#21,ID_REGIONA#22,CO_REGIONA#23,ID_MUNICIP#24,CO_MUN_NOT#25,ID_UNIDADE#26,CO_UNI_NOT#27,CS_SEXO#28,DT_NASC#29,NU_IDADE_N#30,TP_IDADE#31,COD_IDADE#32,CS_GESTANT#33,CS_RACA#34,CS_ESCOL_N#35,ID_PAIS#36,CO_PAIS#37,SG_UF#38,ID_RG_RESI#39,CO_RG_RESI#40,... 142 more fields] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32md:\\ECD-TCC-Dados\\work\\notebooks\\main.ipynb Cell 8\u001B[0m line \u001B[0;36m3\n\u001B[0;32m      <a href='vscode-notebook-cell:/d%3A/ECD-TCC-Dados/work/notebooks/main.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001B[0m \u001B[39m# Valores de camadas inciais e finais para o MLP\u001B[39;00m\n\u001B[0;32m      <a href='vscode-notebook-cell:/d%3A/ECD-TCC-Dados/work/notebooks/main.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001B[0m features_n \u001B[39m=\u001B[39m \u001B[39mlen\u001B[39m(select_cols) \u001B[39m-\u001B[39m \u001B[39m1\u001B[39m\n\u001B[1;32m----> <a href='vscode-notebook-cell:/d%3A/ECD-TCC-Dados/work/notebooks/main.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m classes_n \u001B[39m=\u001B[39m df\u001B[39m.\u001B[39;49mselect(\u001B[39m\"\u001B[39;49m\u001B[39mEVOLUCAO\u001B[39;49m\u001B[39m\"\u001B[39;49m)\u001B[39m.\u001B[39mdistinct()\u001B[39m.\u001B[39mcount() \u001B[39m+\u001B[39m \u001B[39m1\u001B[39m\n",
      "File \u001B[1;32mc:\\Users\\taian\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1421\u001B[0m, in \u001B[0;36mDataFrame.select\u001B[1;34m(self, *cols)\u001B[0m\n\u001B[0;32m   1405\u001B[0m \u001B[39m@ignore_unicode_prefix\u001B[39m\n\u001B[0;32m   1406\u001B[0m \u001B[39m@since\u001B[39m(\u001B[39m1.3\u001B[39m)\n\u001B[0;32m   1407\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mselect\u001B[39m(\u001B[39mself\u001B[39m, \u001B[39m*\u001B[39mcols):\n\u001B[0;32m   1408\u001B[0m \u001B[39m    \u001B[39m\u001B[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001B[39;00m\n\u001B[0;32m   1409\u001B[0m \n\u001B[0;32m   1410\u001B[0m \u001B[39m    :param cols: list of column names (string) or expressions (:class:`Column`).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1419\u001B[0m \u001B[39m    [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\u001B[39;00m\n\u001B[0;32m   1420\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1421\u001B[0m     jdf \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_jdf\u001B[39m.\u001B[39;49mselect(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_jcols(\u001B[39m*\u001B[39;49mcols))\n\u001B[0;32m   1422\u001B[0m     \u001B[39mreturn\u001B[39;00m DataFrame(jdf, \u001B[39mself\u001B[39m\u001B[39m.\u001B[39msql_ctx)\n",
      "File \u001B[1;32mc:\\Users\\taian\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\py4j\\java_gateway.py:1304\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1298\u001B[0m command \u001B[39m=\u001B[39m proto\u001B[39m.\u001B[39mCALL_COMMAND_NAME \u001B[39m+\u001B[39m\\\n\u001B[0;32m   1299\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcommand_header \u001B[39m+\u001B[39m\\\n\u001B[0;32m   1300\u001B[0m     args_command \u001B[39m+\u001B[39m\\\n\u001B[0;32m   1301\u001B[0m     proto\u001B[39m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1303\u001B[0m answer \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mgateway_client\u001B[39m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1304\u001B[0m return_value \u001B[39m=\u001B[39m get_return_value(\n\u001B[0;32m   1305\u001B[0m     answer, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mgateway_client, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtarget_id, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mname)\n\u001B[0;32m   1307\u001B[0m \u001B[39mfor\u001B[39;00m temp_arg \u001B[39min\u001B[39;00m temp_args:\n\u001B[0;32m   1308\u001B[0m     temp_arg\u001B[39m.\u001B[39m_detach()\n",
      "File \u001B[1;32mc:\\Users\\taian\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\sql\\utils.py:134\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    130\u001B[0m converted \u001B[39m=\u001B[39m convert_exception(e\u001B[39m.\u001B[39mjava_exception)\n\u001B[0;32m    131\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    132\u001B[0m     \u001B[39m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    133\u001B[0m     \u001B[39m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 134\u001B[0m     raise_from(converted)\n\u001B[0;32m    135\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    136\u001B[0m     \u001B[39mraise\u001B[39;00m\n",
      "File \u001B[1;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[1;34m(e)\u001B[0m\n",
      "\u001B[1;31mAnalysisException\u001B[0m: Column 'EVOLUCAO' does not exist. Did you mean one of the following? [HEPATICA, VACINA, ASMA, NEUROLOGIC, RENAL, DIABETES, HEMATOLOGI, OBESIDADE, PUERPERA, VACINA_COV, label, CARDIOPATI, IMUNODEPRE, PNEUMOPATI, SIND_DOWN];\n'Project ['EVOLUCAO]\n+- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, VACINA_COV#632, VACINA#648, EVOLUCAO#664 AS label#680]\n   +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, VACINA_COV#632, VACINA#648, CASE WHEN (EVOLUCAO#126 = 1) THEN Y WHEN (EVOLUCAO#126 = 2) THEN N END AS EVOLUCAO#664]\n      +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, VACINA_COV#632, CASE WHEN (VACINA#73 = 1) THEN Y WHEN (VACINA#73 = 2) THEN N END AS VACINA#648, EVOLUCAO#126]\n         +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, OBESIDADE#616, CASE WHEN (VACINA_COV#171 = 1) THEN Y WHEN (VACINA_COV#171 = 2) THEN N END AS VACINA_COV#632, VACINA#73, EVOLUCAO#126]\n            +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, RENAL#600, CASE WHEN (OBESIDADE#69 = 1) THEN Y WHEN (OBESIDADE#69 = 2) THEN N END AS OBESIDADE#616, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n               +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, IMUNODEPRE#584, CASE WHEN (RENAL#68 = 1) THEN Y WHEN (RENAL#68 = 2) THEN N END AS RENAL#600, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                  +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, PNEUMOPATI#568, CASE WHEN (IMUNODEPRE#67 = 1) THEN Y WHEN (IMUNODEPRE#67 = 2) THEN N END AS IMUNODEPRE#584, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                     +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, NEUROLOGIC#552, CASE WHEN (PNEUMOPATI#66 = 1) THEN Y WHEN (PNEUMOPATI#66 = 2) THEN N END AS PNEUMOPATI#568, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                        +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, DIABETES#536, CASE WHEN (NEUROLOGIC#65 = 1) THEN Y WHEN (NEUROLOGIC#65 = 2) THEN N END AS NEUROLOGIC#552, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                           +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, ASMA#520, CASE WHEN (DIABETES#64 = 1) THEN Y WHEN (DIABETES#64 = 2) THEN N END AS DIABETES#536, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                              +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, HEPATICA#504, CASE WHEN (ASMA#63 = 1) THEN Y WHEN (ASMA#63 = 2) THEN N END AS ASMA#520, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                 +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, SIND_DOWN#488, CASE WHEN (HEPATICA#62 = 1) THEN Y WHEN (HEPATICA#62 = 2) THEN N END AS HEPATICA#504, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                    +- Project [PUERPERA#440, CARDIOPATI#456, HEMATOLOGI#472, CASE WHEN (SIND_DOWN#61 = 1) THEN Y WHEN (SIND_DOWN#61 = 2) THEN N END AS SIND_DOWN#488, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                       +- Project [PUERPERA#440, CARDIOPATI#456, CASE WHEN (HEMATOLOGI#60 = 1) THEN Y WHEN (HEMATOLOGI#60 = 2) THEN N END AS HEMATOLOGI#472, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                          +- Project [PUERPERA#440, CASE WHEN (CARDIOPATI#59 = 1) THEN Y WHEN (CARDIOPATI#59 = 2) THEN N END AS CARDIOPATI#456, HEMATOLOGI#60, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                             +- Project [CASE WHEN (PUERPERA#57 = 1) THEN Y WHEN (PUERPERA#57 = 2) THEN N END AS PUERPERA#440, CARDIOPATI#59, HEMATOLOGI#60, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                                +- Filter NOT (VACINA_COV#171 = 12/02/2021)\n                                                   +- Project [PUERPERA#57, CARDIOPATI#59, HEMATOLOGI#60, SIND_DOWN#61, HEPATICA#62, ASMA#63, DIABETES#64, NEUROLOGIC#65, PNEUMOPATI#66, IMUNODEPRE#67, RENAL#68, OBESIDADE#69, VACINA_COV#171, VACINA#73, EVOLUCAO#126]\n                                                      +- Filter (cast(CLASSI_FIN#123 as int) = 5)\n                                                         +- Relation [DT_NOTIFIC#17,SEM_NOT#18,DT_SIN_PRI#19,SEM_PRI#20,SG_UF_NOT#21,ID_REGIONA#22,CO_REGIONA#23,ID_MUNICIP#24,CO_MUN_NOT#25,ID_UNIDADE#26,CO_UNI_NOT#27,CS_SEXO#28,DT_NASC#29,NU_IDADE_N#30,TP_IDADE#31,COD_IDADE#32,CS_GESTANT#33,CS_RACA#34,CS_ESCOL_N#35,ID_PAIS#36,CO_PAIS#37,SG_UF#38,ID_RG_RESI#39,CO_RG_RESI#40,... 142 more fields] csv\n"
     ]
    }
   ],
   "source": [
    "# Valores de camadas inciais e finais para o MLP\n",
    "features_n = len(select_cols) - 1\n",
    "classes_n = df.select(\"EVOLUCAO\").distinct().count() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:37.196271800Z",
     "start_time": "2023-08-26T14:29:37.075988200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type string of column PUERPERA is not supported.\nData type string of column CARDIOPATI is not supported.\nData type string of column HEMATOLOGI is not supported.\nData type string of column SIND_DOWN is not supported.\nData type string of column HEPATICA is not supported.\nData type string of column ASMA is not supported.\nData type string of column DIABETES is not supported.\nData type string of column NEUROLOGIC is not supported.\nData type string of column PNEUMOPATI is not supported.\nData type string of column IMUNODEPRE is not supported.\nData type string of column RENAL is not supported.\nData type string of column OBESIDADE is not supported.\nData type string of column VACINA_COV is not supported.\nData type string of column VACINA is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[27], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[39m# Juntar as variáveis em um vetor de features\u001B[39;00m\n\u001B[0;32m      2\u001B[0m assembler \u001B[39m=\u001B[39m VectorAssembler(inputCols\u001B[39m=\u001B[39mfeature_list, outputCol\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39mfeatures\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m assembledDF \u001B[39m=\u001B[39m assembler\u001B[39m.\u001B[39;49mtransform(df)\n",
      "File \u001B[1;32mc:\\Users\\taian\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\base.py:170\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[1;34m(self, dataset, params)\u001B[0m\n\u001B[0;32m    168\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcopy(params)\u001B[39m.\u001B[39m_transform(dataset)\n\u001B[0;32m    169\u001B[0m     \u001B[39melse\u001B[39;00m:\n\u001B[1;32m--> 170\u001B[0m         \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_transform(dataset)\n\u001B[0;32m    171\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    172\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mValueError\u001B[39;00m(\u001B[39m\"\u001B[39m\u001B[39mParams must be a param map but got \u001B[39m\u001B[39m%s\u001B[39;00m\u001B[39m.\u001B[39m\u001B[39m\"\u001B[39m \u001B[39m%\u001B[39m \u001B[39mtype\u001B[39m(params))\n",
      "File \u001B[1;32mc:\\Users\\taian\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\wrapper.py:338\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    336\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39m_transform\u001B[39m(\u001B[39mself\u001B[39m, dataset):\n\u001B[0;32m    337\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_transfer_params_to_java()\n\u001B[1;32m--> 338\u001B[0m     \u001B[39mreturn\u001B[39;00m DataFrame(\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_java_obj\u001B[39m.\u001B[39;49mtransform(dataset\u001B[39m.\u001B[39;49m_jdf), dataset\u001B[39m.\u001B[39msql_ctx)\n",
      "File \u001B[1;32mc:\\Users\\taian\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\py4j\\java_gateway.py:1304\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1298\u001B[0m command \u001B[39m=\u001B[39m proto\u001B[39m.\u001B[39mCALL_COMMAND_NAME \u001B[39m+\u001B[39m\\\n\u001B[0;32m   1299\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcommand_header \u001B[39m+\u001B[39m\\\n\u001B[0;32m   1300\u001B[0m     args_command \u001B[39m+\u001B[39m\\\n\u001B[0;32m   1301\u001B[0m     proto\u001B[39m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1303\u001B[0m answer \u001B[39m=\u001B[39m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mgateway_client\u001B[39m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1304\u001B[0m return_value \u001B[39m=\u001B[39m get_return_value(\n\u001B[0;32m   1305\u001B[0m     answer, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mgateway_client, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mtarget_id, \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mname)\n\u001B[0;32m   1307\u001B[0m \u001B[39mfor\u001B[39;00m temp_arg \u001B[39min\u001B[39;00m temp_args:\n\u001B[0;32m   1308\u001B[0m     temp_arg\u001B[39m.\u001B[39m_detach()\n",
      "File \u001B[1;32mc:\\Users\\taian\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\sql\\utils.py:134\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    130\u001B[0m converted \u001B[39m=\u001B[39m convert_exception(e\u001B[39m.\u001B[39mjava_exception)\n\u001B[0;32m    131\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mnot\u001B[39;00m \u001B[39misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    132\u001B[0m     \u001B[39m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    133\u001B[0m     \u001B[39m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 134\u001B[0m     raise_from(converted)\n\u001B[0;32m    135\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[0;32m    136\u001B[0m     \u001B[39mraise\u001B[39;00m\n",
      "File \u001B[1;32m<string>:3\u001B[0m, in \u001B[0;36mraise_from\u001B[1;34m(e)\u001B[0m\n",
      "\u001B[1;31mIllegalArgumentException\u001B[0m: Data type string of column PUERPERA is not supported.\nData type string of column CARDIOPATI is not supported.\nData type string of column HEMATOLOGI is not supported.\nData type string of column SIND_DOWN is not supported.\nData type string of column HEPATICA is not supported.\nData type string of column ASMA is not supported.\nData type string of column DIABETES is not supported.\nData type string of column NEUROLOGIC is not supported.\nData type string of column PNEUMOPATI is not supported.\nData type string of column IMUNODEPRE is not supported.\nData type string of column RENAL is not supported.\nData type string of column OBESIDADE is not supported.\nData type string of column VACINA_COV is not supported.\nData type string of column VACINA is not supported."
     ]
    }
   ],
   "source": [
    "# Juntar as variáveis em um vetor de features\n",
    "assembler = VectorAssembler(inputCols=feature_list, outputCol=\"features\")\n",
    "assembledDF = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:37.254055600Z",
     "start_time": "2023-08-26T14:29:37.201780100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modelos a testar: regressão logística, Floresta aleatória e multilayerperceptron\n",
    "lr = LogisticRegression().setFamily(\"binomial\")\n",
    "rf = RandomForestClassifier()\n",
    "mlp = MultilayerPerceptronClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:37.258053900Z",
     "start_time": "2023-08-26T14:29:37.228832Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pipelines de transformações para a validação cruzada\n",
    "pipeline_lr = Pipeline(stages=[rf])\n",
    "pipeline_rf = Pipeline(stages=[rf])\n",
    "pipeline_mlp = Pipeline(stages=[mlp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:37.268582600Z",
     "start_time": "2023-08-26T14:29:37.246533700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parâmetros para a validação cruzada\n",
    "paramGrid_lr = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5, 1.0, 2.0])\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "    .addGrid(lr.maxIter, [1, 5, 10, 20, 50])\n",
    "    .build()\n",
    ")\n",
    "paramGrid_rf = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(rf.numTrees, [10, 30, 50])\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15])\n",
    "    .build()\n",
    ")\n",
    "paramGrid_mlp = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(mlp.layers, [\n",
    "        [features_n, 3, 3, classes_n],\n",
    "        [features_n, 4, 4, classes_n],\n",
    "        [features_n, 3, 3, 3, classes_n],\n",
    "        [features_n, 4, 4, 4, classes_n],\n",
    "    ]).build()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:37.307706500Z",
     "start_time": "2023-08-26T14:29:37.261561900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validador cruzado\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=paramGrid_lr,\n",
    "                             evaluator=MulticlassClassificationEvaluator())\n",
    "crossval_rf = CrossValidator(estimator=pipeline_rf, estimatorParamMaps=paramGrid_rf,\n",
    "                             evaluator=MulticlassClassificationEvaluator())\n",
    "crossval_mlp = CrossValidator(estimator=pipeline_mlp, estimatorParamMaps=paramGrid_mlp,\n",
    "                              evaluator=MulticlassClassificationEvaluator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:37.347650400Z",
     "start_time": "2023-08-26T14:29:37.307706500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separar os dados em treino e teste\n",
    "taxa_de_treino = 0.00001\n",
    "(trainingData, testData) = df.randomSplit([taxa_de_treino, 1 - taxa_de_treino])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:38.575492400Z",
     "start_time": "2023-08-26T14:29:37.338284800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Salvando as massa utilizadas em treino e teste para fazer análises depois de gerar os modelos\n",
    "trainingData.write.mode(\"overwrite\").parquet(\"../datasets/training\")\n",
    "testData.write.mode(\"overwrite\").parquet(\"../datasets/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:40.102194800Z",
     "start_time": "2023-08-26T14:29:38.576491800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o11341.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 477.0 failed 1 times, most recent failure: Lost task 0.0 in stage 477.0 (TID 2762) (192.168.1.113 executor driver): scala.MatchError: [null,1.0,[-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,1.0,-1.0,-1.0,-1.0,-1.0,-1.0,0.0,0.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1470)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1443)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: scala.MatchError: [null,1.0,[-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,1.0,-1.0,-1.0,-1.0,-1.0,-1.0,0.0,0.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m crossval_lr_model \u001B[38;5;241m=\u001B[39m \u001B[43mcrossval_lr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43massembledDF\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[1;34m(self, dataset, params)\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[0;32m    210\u001B[0m     )\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\tuning.py:847\u001B[0m, in \u001B[0;36mCrossValidator._fit\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    841\u001B[0m train \u001B[38;5;241m=\u001B[39m datasets[i][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcache()\n\u001B[0;32m    843\u001B[0m tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[0;32m    844\u001B[0m     inheritable_thread_target,\n\u001B[0;32m    845\u001B[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[0;32m    846\u001B[0m )\n\u001B[1;32m--> 847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: f(), tasks):\n\u001B[0;32m    848\u001B[0m     metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n\u001B[0;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m collectSubModelsParam:\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\multiprocessing\\pool.py:873\u001B[0m, in \u001B[0;36mIMapIterator.next\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    871\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[0;32m    872\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m value\n\u001B[1;32m--> 873\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m value\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\multiprocessing\\pool.py:125\u001B[0m, in \u001B[0;36mworker\u001B[1;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001B[0m\n\u001B[0;32m    123\u001B[0m job, i, func, args, kwds \u001B[38;5;241m=\u001B[39m task\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 125\u001B[0m     result \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28;01mTrue\u001B[39;00m, func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds))\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    127\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m wrap_exception \u001B[38;5;129;01mand\u001B[39;00m func \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _helper_reraises_exception:\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\tuning.py:847\u001B[0m, in \u001B[0;36mCrossValidator._fit.<locals>.<lambda>\u001B[1;34m(f)\u001B[0m\n\u001B[0;32m    841\u001B[0m train \u001B[38;5;241m=\u001B[39m datasets[i][\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcache()\n\u001B[0;32m    843\u001B[0m tasks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmap\u001B[39m(\n\u001B[0;32m    844\u001B[0m     inheritable_thread_target,\n\u001B[0;32m    845\u001B[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001B[0;32m    846\u001B[0m )\n\u001B[1;32m--> 847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j, metric, subModel \u001B[38;5;129;01min\u001B[39;00m pool\u001B[38;5;241m.\u001B[39mimap_unordered(\u001B[38;5;28;01mlambda\u001B[39;00m f: \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, tasks):\n\u001B[0;32m    848\u001B[0m     metrics_all[i][j] \u001B[38;5;241m=\u001B[39m metric\n\u001B[0;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m collectSubModelsParam:\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\util.py:337\u001B[0m, in \u001B[0;36minheritable_thread_target.<locals>.wrapped\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    335\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    336\u001B[0m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_jsc\u001B[38;5;241m.\u001B[39msc()\u001B[38;5;241m.\u001B[39msetLocalProperties(properties)\n\u001B[1;32m--> 337\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\tuning.py:113\u001B[0m, in \u001B[0;36m_parallelFitTasks.<locals>.singleTask\u001B[1;34m()\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msingleTask\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m, Transformer]:\n\u001B[1;32m--> 113\u001B[0m     index, model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodelIter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001B[39;00m\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001B[39;00m\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;66;03m#  all nested stages and evaluators\u001B[39;00m\n\u001B[0;32m    118\u001B[0m     metric \u001B[38;5;241m=\u001B[39m eva\u001B[38;5;241m.\u001B[39mevaluate(model\u001B[38;5;241m.\u001B[39mtransform(validation, epm[index]))\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\base.py:98\u001B[0m, in \u001B[0;36m_FitMultipleIterator.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     96\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo models remaining.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcounter \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m index, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfitSingleModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\base.py:156\u001B[0m, in \u001B[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001B[1;34m(index)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfitSingleModel\u001B[39m(index: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m M:\n\u001B[1;32m--> 156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparamMaps\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\base.py:203\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[1;34m(self, dataset, params)\u001B[0m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(params, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m params:\n\u001B[1;32m--> 203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(dataset)\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    132\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m stage\u001B[38;5;241m.\u001B[39mtransform(dataset)\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# must be an Estimator\u001B[39;00m\n\u001B[1;32m--> 134\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mstage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m     transformers\u001B[38;5;241m.\u001B[39mappend(model)\n\u001B[0;32m    136\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m indexOfLastEstimator:\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[1;34m(self, dataset, params)\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[0;32m    210\u001B[0m     )\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[1;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[1;34m(self, dataset)\u001B[0m\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[1;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 169\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    170\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    171\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\.virtualenvs\\ECD-TCC-Dados-cYnMm7Sm\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o11341.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 477.0 failed 1 times, most recent failure: Lost task 0.0 in stage 477.0 (TID 2762) (192.168.1.113 executor driver): scala.MatchError: [null,1.0,[-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,1.0,-1.0,-1.0,-1.0,-1.0,-1.0,0.0,0.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1470)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1443)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:119)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:161)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:138)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:115)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor131.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: scala.MatchError: [null,1.0,[-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,1.0,-1.0,-1.0,-1.0,-1.0,-1.0,0.0,0.0]] (of class org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema)\r\n\tat org.apache.spark.ml.PredictorParams.$anonfun$extractInstances$1(Predictor.scala:81)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$SliceIterator.next(Iterator.scala:273)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1431)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$2(RDD.scala:1470)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "crossval_lr_model = crossval_lr.fit(assembledDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:40.109203900Z",
     "start_time": "2023-08-26T14:29:40.105203400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crossval_lr_model.write().save(\"model/lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:40.110200900Z",
     "start_time": "2023-08-26T14:29:40.109203900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Avaliando o modelo\n",
    "predictions = cvModel.transform(testData)\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "predictions.select(\"label\", \"features\", \"rawPrediction\",\n",
    "                   \"probability\", \"prediction\").show()\n",
    "predictions.select(\"prediction\").distinct().show()\n",
    "\n",
    "result = predictions.toPandas()\n",
    "\n",
    "plt.plot(result.label, result.prediction, 'bo')\n",
    "plt.xlabel('Sobrevivencia')\n",
    "plt.ylabel('Prediction')\n",
    "plt.suptitle(\"Model Performance RMSE: %f\" % rmse)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-26T14:29:40.111712900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selecionando o melhor modelo\n",
    "bestPipeline = cvModel.bestModel\n",
    "bestModel = bestPipeline.stages[2]\n",
    "\n",
    "importances = bestModel.featureImportances\n",
    "\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "plt.bar(x_values, importances, orientation='vertical')\n",
    "plt.xticks(x_values, feature_list, rotation=40)\n",
    "plt.ylabel('Importance')\n",
    "plt.xlabel('Feature')\n",
    "plt.title('Feature Importances')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-26T14:29:40.115723200Z",
     "start_time": "2023-08-26T14:29:40.114724500Z"
    }
   },
   "outputs": [],
   "source": [
    "print('numTrees - ', bestModel.getNumTrees)\n",
    "print('maxDepth - ', bestModel.getOrDefault('maxDepth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-26T14:29:40.117722600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "f708a36acfaef0acf74ccd43dfb58100269bf08fb79032a1e0a6f35bd9856f51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
